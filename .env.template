# MATLAB2C++ Agent Environment Configuration Template
# Copy this file to .env and modify the values according to your setup

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# LLM Provider: 'vllm', 'openai', 'anthropic', etc.
LLM_PROVIDER=vllm

# =============================================================================
# vLLM Configuration (when LLM_PROVIDER=vllm)
# =============================================================================

# vLLM Server Endpoint
VLLM_ENDPOINT=http://localhost:8000

# vLLM Model Name (used for model identification)
VLLM_MODEL_NAME=Qwen/Qwen3-32B-FP8

# vLLM API Key (if required)
VLLM_API_KEY=dummy_key

# =============================================================================
# OpenAI Configuration (when LLM_PROVIDER=openai)
# =============================================================================

# OpenAI API Key
OPENAI_API_KEY=your-openai-api-key-here

# OpenAI Model Name
OPENAI_MODEL=gpt-4

# OpenAI Base URL (optional, for custom endpoints)
OPENAI_BASE_URL=https://api.openai.com/v1

# =============================================================================
# General LLM Settings
# =============================================================================

# Base URL for LLM API requests (auto-configured for vLLM)
LLM_BASE_URL=http://localhost:8000/v1

# Model name to use (overrides provider-specific settings)
LLM_MODEL=Qwen/Qwen3-32B-FP8

# API Key for LLM service (overrides provider-specific settings)
LLM_API_KEY=dummy_key

# Request timeout in seconds
LLM_TIMEOUT=1200

# Maximum tokens per request
LLM_MAX_TOKENS=8000

# Temperature for LLM responses (0.0-1.0)
LLM_TEMPERATURE=0.1

# =============================================================================
# Project Configuration
# =============================================================================

# Default output directory for generated C++ projects
DEFAULT_OUTPUT_DIR=./output

# Default C++ standard to use
DEFAULT_CPP_STANDARD=C++17

# Default maximum optimization turns
DEFAULT_MAX_TURNS=2

# Default target quality score (0-10)
DEFAULT_TARGET_QUALITY=7.0

# Whether to include tests by default
DEFAULT_INCLUDE_TESTS=true

# =============================================================================
# Analysis Configuration
# =============================================================================

# Maximum file size to analyze (bytes)
MAX_FILE_SIZE=100000

# Chunk size for large files
CHUNK_SIZE=8000

# Number of analysis passes
ANALYSIS_PASSES=3

# Minimum confidence threshold for analysis
CONFIDENCE_THRESHOLD=0.7

# =============================================================================
# Conversion Configuration
# =============================================================================

# Include optimization suggestions
INCLUDE_OPTIMIZATION=true

# Generate unit tests
GENERATE_TESTS=true

# Create documentation
CREATE_DOCUMENTATION=true

# =============================================================================
# Logging Configuration
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable console logging
ENABLE_CONSOLE_LOGGING=true

# Log file path (optional)
LOG_FILE=./logs/matlab2cpp.log

# =============================================================================
# Development Configuration
# =============================================================================

# Enable development mode (additional debugging, verbose output)
DEV_MODE=false

# Enable performance profiling
ENABLE_PROFILING=false

# Cache directory for temporary files
CACHE_DIR=./.cache

# =============================================================================
# Example Configurations for Different Scenarios
# =============================================================================

# For local vLLM server:
# VLLM_ENDPOINT=http://localhost:8000
# VLLM_MODEL_NAME=Qwen/Qwen3-32B-FP8
# LLM_PROVIDER=vllm

# For remote vLLM server:
# VLLM_ENDPOINT=http://192.168.6.10:8002
# VLLM_MODEL_NAME=Qwen/Qwen3-32B-FP8
# LLM_PROVIDER=vllm

# For OpenAI:
# OPENAI_API_KEY=sk-your-key-here
# OPENAI_MODEL=gpt-4
# LLM_PROVIDER=openai

# For Anthropic Claude:
# ANTHROPIC_API_KEY=your-key-here
# ANTHROPIC_MODEL=claude-3-opus-20240229
# LLM_PROVIDER=anthropic
